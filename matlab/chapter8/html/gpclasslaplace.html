
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>gpclasslaplace.m</title><meta name="generator" content="MATLAB 8.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2016-08-31"><meta name="DC.source" content="gpclasslaplace.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>gpclasslaplace.m</h1><!--introduction--><p>Demomnstrates making predictions using the Laplace approximation for GP classification for 1- and 2-D data.</p><p>From A First Course in Machine Learning Simon Rogers, August 2016 [simon.rogers@glasgow.ac.uk]</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Full Laplace predictions - 1 dimensional data</a></li><li><a href="#2">Setup things required -- taken from gpclass.m</a></li><li><a href="#3">Generative model - Figure 8.18 in book</a></li><li><a href="#4">Generate samples from the Laplace approximation and make predictions</a></li><li><a href="#5">Laplace approximation for the 2D example</a></li><li><a href="#6">create some random data and then change the means of the two classes to</a></li><li><a href="#7">Full Laplace approximation - compute the test covariance</a></li><li><a href="#8">Generate 3 sample functions from the Laplace and plot the resulting decision boundaries</a></li><li><a href="#9">Average over lots of samples from the Laplace</a></li></ul></div><h2>Full Laplace predictions - 1 dimensional data<a name="1"></a></h2><p>We can now look at the full Laplace approximation. Here we sample from the full multivariate Gaussian defined by the Laplace approximation Note that the first cell duplicates lots of things from gpclass.m</p><h2>Setup things required -- taken from gpclass.m<a name="2"></a></h2><pre class="codeinput">clear <span class="string">all</span>;close <span class="string">all</span>;

<span class="comment">% If you don't want the exact data from the book, comment the rng line</span>
<span class="comment">% and the two that slightly change x values</span>
rng(100);
N = 10;
x = rand(N,1);
x = sort(x);
x(3) = x(3) + 0.05;
x(10) = x(10) + 0.05;


<span class="comment">% Set the covariance parameters</span>
gamma = 10;
alpha = 10;

<span class="comment">% Compute the covariance matrix (training)</span>
<span class="keyword">for</span> n = 1:N
    <span class="keyword">for</span> m = 1:N
        C(n,m) = alpha*exp(-gamma*(x(n)-x(m))^2);
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><h2>Generative model - Figure 8.18 in book<a name="3"></a></h2><p>Change the rng parameter to get different datasets</p><pre class="codeinput">rng(84)
<span class="comment">% Sample a function from the GP prior</span>
f = mvnrnd(repmat(0,N,1),C);
p = 1./(1+exp(-f));
u = rand(N,1);
t = zeros(N,1);
t(u&lt;=p') = 1;

<span class="comment">% Newton-Rapshon procedure for finding the MAP estimate of f</span>
f = zeros(N,1);
invC = inv(C);

<span class="comment">% Newton-Raphson procedure (p.300)</span>
max_its = 10;
it = 1;
allf = zeros(max_its,N);
allf(1,:) = f';
<span class="keyword">while</span> it &lt; max_its
    g = 1./(1+exp(-f));
    gradient = -invC*f + t - g;
    hessian = -invC - diag(g.*(1-g));
    f = f - inv(hessian)*gradient;
    it = it + 1;
    allf(it,:) = f';
<span class="keyword">end</span>
hessian = -invC - diag(g.*(1-g));


<span class="comment">% Predictions with the point estimate</span>
<span class="comment">% Define some test points for vidualisation and compute the test covariance</span>
testx = [0:0.01:1]';
Ntest = length(testx);
<span class="comment">% Compute the required covariance functions</span>
<span class="keyword">for</span> n = 1:N
    <span class="keyword">for</span> m = 1:Ntest
        R(n,m) = alpha*exp(-gamma*(x(n)-testx(m))^2);
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="keyword">for</span> n = 1:Ntest
    <span class="keyword">for</span> m = 1:Ntest
        Cstar(n,m) = alpha*exp(-gamma*(testx(n) - testx(m))^2);
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">% Compute the latent function at the test points</span>
fs = R'*invC*f;

<span class="comment">% Compute the posterior covariance</span>
covf = -inv(hessian);

<span class="comment">% Compute the predictive mean and covariance</span>
pred_mu = R'*invC*f;
pred_cov = Cstar - R'*(C\R) + R'*(C\covf)*invC*R;
pred_cov = pred_cov + 1e-6*eye(Ntest);
</pre><h2>Generate samples from the Laplace approximation and make predictions<a name="4"></a></h2><pre class="codeinput">figure()
hold <span class="string">off</span>
<span class="comment">% Generate 1000 samples from the Laplace approximation</span>
samps = gausssamp(pred_mu,pred_cov,1000);
<span class="comment">% Convert into probabilities and compute mean</span>
p = 1./(1+exp(-samps));
p = mean(p,1);
<span class="comment">% Plot the Laplace predictions</span>
plot(testx,p,<span class="string">'k'</span>,<span class="string">'linewidth'</span>,2)
hold <span class="string">on</span>
<span class="comment">% Plot the point approximation for comparison</span>
plot(testx,1./(1+exp(-fs)),<span class="string">'k--'</span>)
pos = find(t==0);
plot(x(pos),1./(1+exp(-f(pos))),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,15,<span class="string">'markerfacecolor'</span>,[0.6 0.6 0.6])
pos = find(t==1);
plot(x(pos),1./(1+exp(-f(pos))),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,15,<span class="string">'markerfacecolor'</span>,[1 1 1])
xlabel(<span class="string">'x'</span>)
ylabel(<span class="string">'P(T=1|x)'</span>)
</pre><img vspace="5" hspace="5" src="gpclasslaplace_01.png" alt=""> <h2>Laplace approximation for the 2D example<a name="5"></a></h2><p>Again, the first cell is setting things up, taken from class2d.m</p><h2>create some random data and then change the means of the two classes to<a name="6"></a></h2><p>seeparate them</p><pre class="codeinput">rng(2)
x = randn(20,2);
x(1:10,:) = x(1:10,:) - 2;
x(11:end,:) = x(11:end,:) + 2;
t = [repmat(0,10,1);repmat(1,10,1)];


<span class="comment">% Set the GP hyperparameters and compute the covariance function</span>

alpha = 10;
gamma = 0.1;
N = size(x,1);
C = zeros(N);
<span class="keyword">for</span> n = 1:N
    <span class="keyword">for</span> m = 1:N
        C(n,m) = alpha*exp(-gamma*sum((x(n,:)-x(m,:)).^2));
    <span class="keyword">end</span>
<span class="keyword">end</span>


<span class="comment">% Newton-raphson procedure to optimise the latent function values</span>
<span class="comment">% Initialise all to zero</span>
f = repmat(0,N,1);
allf = [f'];
<span class="comment">% Pre-compute the inverse of C</span>
invC = inv(C);
<span class="keyword">for</span> iteration = 2:6
    g = 1./(1+exp(-f));
    gra = t - g - invC*f;
    H = -diag(g.*(1-g)) - invC;
    f = f - inv(H)*gra;
    allf(iteration,:) = f';
<span class="keyword">end</span>
H = -diag(g.*(1-g)) - invC;
<span class="comment">% Plot the evolution of the f values</span>

<span class="comment">% Visualise the predictive function via a large grid of test points</span>
<span class="comment">% Create the grid</span>
[X,Y] = meshgrid(-4.5:0.3:4.5,-4.5:0.3:4.5);
testN = prod(size(X));
testX = [reshape(X,testN,1) reshape(Y,testN,1)];
<span class="comment">% Create the test covariance function</span>
R = zeros(N,testN);
<span class="keyword">for</span> n = 1:N
    <span class="keyword">for</span> m = 1:testN
        R(n,m) = alpha*exp(-gamma*sum((x(n,:) - testX(m,:)).^2));
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><h2>Full Laplace approximation - compute the test covariance<a name="7"></a></h2><pre class="codeinput">Cstar = zeros(testN);
<span class="keyword">for</span> n = 1:testN
    <span class="keyword">for</span> m = 1:testN
        Cstar(n,m) = alpha*exp(-gamma*sum((testX(n,:) - testX(m,:)).^2));
    <span class="keyword">end</span>
<span class="keyword">end</span>
covf = -inv(H);
pred_mu = R'*invC*f;
pred_cov = Cstar - R'*(C\R) + R'*(C\covf)*invC*R;
pred_cov = pred_cov + 1e-6*eye(testN);
</pre><h2>Generate 3 sample functions from the Laplace and plot the resulting decision boundaries<a name="8"></a></h2><p>change rng for different samples</p><pre class="codeinput">rng(11)
figure()
hold <span class="string">off</span>
n_samps = 3;
samps = gausssamp(pred_mu,pred_cov,n_samps);
p = 1./(1+exp(-samps));
styles = {<span class="string">'k'</span>,<span class="string">'k--'</span>,<span class="string">'k-.'</span>};
<span class="keyword">for</span> i = 1:n_samps
    contour(X,Y,reshape(p(i,:),size(X)),[0.5 0.5],styles{i},<span class="string">'linewidth'</span>,2)
    hold <span class="string">on</span>
<span class="keyword">end</span>
pos = find(t==0);
plot(x(pos,1),x(pos,2),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2,<span class="string">'markerfacecolor'</span>,[0.6 0.6 0.6])
pos = find(t==1);
plot(x(pos,1),x(pos,2),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2,<span class="string">'markerfacecolor'</span>,[1 1 1])
xlabel(<span class="string">'$x_1$'</span>,<span class="string">'interpreter'</span>,<span class="string">'latex'</span>)
ylabel(<span class="string">'$x_2$'</span>,<span class="string">'interpreter'</span>,<span class="string">'latex'</span>)
</pre><img vspace="5" hspace="5" src="gpclasslaplace_02.png" alt=""> <h2>Average over lots of samples from the Laplace<a name="9"></a></h2><pre class="codeinput">figure()
hold <span class="string">off</span>
n_samps = 1000;
samps = gausssamp(pred_mu,pred_cov,n_samps);
p = 1./(1+exp(-samps));
avgp = mean(p,1);
[c,h] = contour(X,Y,reshape(avgp,size(X)),<span class="string">'k'</span>);
clabel(c,h)
set(h,<span class="string">'linewidth'</span>,2)
hold <span class="string">on</span>
pos = find(t==0);
plot(x(pos,1),x(pos,2),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2,<span class="string">'markerfacecolor'</span>,[0.6 0.6 0.6])
pos = find(t==1);
plot(x(pos,1),x(pos,2),<span class="string">'ko'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2,<span class="string">'markerfacecolor'</span>,[1 1 1])
xlabel(<span class="string">'$x_1$'</span>,<span class="string">'interpreter'</span>,<span class="string">'latex'</span>)
ylabel(<span class="string">'$x_2$'</span>,<span class="string">'interpreter'</span>,<span class="string">'latex'</span>)
</pre><img vspace="5" hspace="5" src="gpclasslaplace_03.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% gpclasslaplace.m
% Demomnstrates making predictions using the Laplace approximation for GP
% classification for 1- and 2-D data.
%
% From A First Course in Machine Learning
% Simon Rogers, August 2016 [simon.rogers@glasgow.ac.uk]
%

%% Full Laplace predictions - 1 dimensional data
% We can now look at the full Laplace approximation. Here we sample from
% the full multivariate Gaussian defined by the Laplace approximation
% Note that the first cell duplicates lots of things from gpclass.m


%% Setup things required REPLACE_WITH_DASH_DASH taken from gpclass.m
clear all;close all;

% If you don't want the exact data from the book, comment the rng line
% and the two that slightly change x values
rng(100);
N = 10;
x = rand(N,1);
x = sort(x);
x(3) = x(3) + 0.05;
x(10) = x(10) + 0.05;


% Set the covariance parameters
gamma = 10;
alpha = 10;

% Compute the covariance matrix (training)
for n = 1:N
    for m = 1:N
        C(n,m) = alpha*exp(-gamma*(x(n)-x(m))^2);
    end
end

%% Generative model - Figure 8.18 in book
% Change the rng parameter to get different datasets
rng(84)
% Sample a function from the GP prior
f = mvnrnd(repmat(0,N,1),C);
p = 1./(1+exp(-f));
u = rand(N,1);
t = zeros(N,1);
t(u<=p') = 1;

% Newton-Rapshon procedure for finding the MAP estimate of f
f = zeros(N,1);
invC = inv(C);

% Newton-Raphson procedure (p.300)
max_its = 10;
it = 1;
allf = zeros(max_its,N);
allf(1,:) = f';
while it < max_its
    g = 1./(1+exp(-f));
    gradient = -invC*f + t - g;
    hessian = -invC - diag(g.*(1-g));
    f = f - inv(hessian)*gradient;
    it = it + 1;
    allf(it,:) = f';
end
hessian = -invC - diag(g.*(1-g));


% Predictions with the point estimate
% Define some test points for vidualisation and compute the test covariance
testx = [0:0.01:1]';
Ntest = length(testx);
% Compute the required covariance functions
for n = 1:N
    for m = 1:Ntest
        R(n,m) = alpha*exp(-gamma*(x(n)-testx(m))^2);
    end
end

for n = 1:Ntest
    for m = 1:Ntest
        Cstar(n,m) = alpha*exp(-gamma*(testx(n) - testx(m))^2);
    end
end

% Compute the latent function at the test points
fs = R'*invC*f;

% Compute the posterior covariance
covf = -inv(hessian);

% Compute the predictive mean and covariance
pred_mu = R'*invC*f;
pred_cov = Cstar - R'*(C\R) + R'*(C\covf)*invC*R;
pred_cov = pred_cov + 1e-6*eye(Ntest);


%% Generate samples from the Laplace approximation and make predictions
figure()
hold off
% Generate 1000 samples from the Laplace approximation
samps = gausssamp(pred_mu,pred_cov,1000);
% Convert into probabilities and compute mean
p = 1./(1+exp(-samps));
p = mean(p,1);
% Plot the Laplace predictions
plot(testx,p,'k','linewidth',2)
hold on
% Plot the point approximation for comparison
plot(testx,1./(1+exp(-fs)),'kREPLACE_WITH_DASH_DASH')
pos = find(t==0);
plot(x(pos),1./(1+exp(-f(pos))),'ko','markersize',15,'markerfacecolor',[0.6 0.6 0.6])
pos = find(t==1);
plot(x(pos),1./(1+exp(-f(pos))),'ko','markersize',15,'markerfacecolor',[1 1 1])
xlabel('x')
ylabel('P(T=1|x)')


%% Laplace approximation for the 2D example
% Again, the first cell is setting things up, taken from class2d.m


%% create some random data and then change the means of the two classes to
% seeparate them
rng(2)
x = randn(20,2);
x(1:10,:) = x(1:10,:) - 2;
x(11:end,:) = x(11:end,:) + 2;
t = [repmat(0,10,1);repmat(1,10,1)];


% Set the GP hyperparameters and compute the covariance function

alpha = 10;
gamma = 0.1;
N = size(x,1);
C = zeros(N);
for n = 1:N
    for m = 1:N
        C(n,m) = alpha*exp(-gamma*sum((x(n,:)-x(m,:)).^2));
    end
end


% Newton-raphson procedure to optimise the latent function values
% Initialise all to zero
f = repmat(0,N,1);
allf = [f'];
% Pre-compute the inverse of C
invC = inv(C);
for iteration = 2:6
    g = 1./(1+exp(-f));
    gra = t - g - invC*f;
    H = -diag(g.*(1-g)) - invC;
    f = f - inv(H)*gra;
    allf(iteration,:) = f';
end
H = -diag(g.*(1-g)) - invC;
% Plot the evolution of the f values

% Visualise the predictive function via a large grid of test points
% Create the grid
[X,Y] = meshgrid(-4.5:0.3:4.5,-4.5:0.3:4.5);
testN = prod(size(X));
testX = [reshape(X,testN,1) reshape(Y,testN,1)];
% Create the test covariance function
R = zeros(N,testN);
for n = 1:N
    for m = 1:testN
        R(n,m) = alpha*exp(-gamma*sum((x(n,:) - testX(m,:)).^2));
    end
end
 
%% Full Laplace approximation - compute the test covariance
Cstar = zeros(testN);
for n = 1:testN
    for m = 1:testN
        Cstar(n,m) = alpha*exp(-gamma*sum((testX(n,:) - testX(m,:)).^2));
    end
end
covf = -inv(H);
pred_mu = R'*invC*f;
pred_cov = Cstar - R'*(C\R) + R'*(C\covf)*invC*R;
pred_cov = pred_cov + 1e-6*eye(testN);

%% Generate 3 sample functions from the Laplace and plot the resulting decision boundaries
% change rng for different samples
rng(11)
figure()
hold off
n_samps = 3;
samps = gausssamp(pred_mu,pred_cov,n_samps);
p = 1./(1+exp(-samps));
styles = {'k','kREPLACE_WITH_DASH_DASH','k-.'};
for i = 1:n_samps
    contour(X,Y,reshape(p(i,:),size(X)),[0.5 0.5],styles{i},'linewidth',2)
    hold on
end
pos = find(t==0);
plot(x(pos,1),x(pos,2),'ko','markersize',10,'linewidth',2,'markerfacecolor',[0.6 0.6 0.6])
pos = find(t==1);
plot(x(pos,1),x(pos,2),'ko','markersize',10,'linewidth',2,'markerfacecolor',[1 1 1])
xlabel('$x_1$','interpreter','latex')
ylabel('$x_2$','interpreter','latex')

%% Average over lots of samples from the Laplace
figure()
hold off
n_samps = 1000;
samps = gausssamp(pred_mu,pred_cov,n_samps);
p = 1./(1+exp(-samps));
avgp = mean(p,1);
[c,h] = contour(X,Y,reshape(avgp,size(X)),'k');
clabel(c,h)
set(h,'linewidth',2)
hold on
pos = find(t==0);
plot(x(pos,1),x(pos,2),'ko','markersize',10,'linewidth',2,'markerfacecolor',[0.6 0.6 0.6])
pos = find(t==1);
plot(x(pos,1),x(pos,2),'ko','markersize',10,'linewidth',2,'markerfacecolor',[1 1 1])
xlabel('$x_1$','interpreter','latex')
ylabel('$x_2$','interpreter','latex')

##### SOURCE END #####
--></body></html>