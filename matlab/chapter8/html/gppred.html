
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>gppred</title><meta name="generator" content="MATLAB 8.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2016-08-30"><meta name="DC.source" content="gppred.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">gpprior.m</a></li><li><a href="#2">Set the kernel parameters</a></li><li><a href="#3">Define x</a></li><li><a href="#4">Create the training covariance matrix</a></li><li><a href="#5">Generate a true function from the GP prior with zero mean</a></li><li><a href="#6">Define the test points - again on a uniform grid</a></li><li><a href="#7">Plot the training data and show the position of the test points</a></li><li><a href="#8">Compute the test covariance</a></li><li><a href="#9">Compute the mean and covariance of the predictions</a></li><li><a href="#10">Plot a smooth predictive function with a one sd window,</a></li><li><a href="#11">Finally, plot some sample functions drawn from the predictive distribution</a></li></ul></div><h2>gpprior.m<a name="1"></a></h2><p>Performs noise-free GP predictions</p><p>From A First Course in Machine Learning Simon Rogers, August 2016 [simon.rogers@glasgow.ac.uk]</p><pre class="codeinput">clear <span class="string">all</span>; close <span class="string">all</span>;
</pre><h2>Set the kernel parameters<a name="2"></a></h2><p>Try varying these to see the effect</p><pre class="codeinput">gamma = 0.5;
alpha = 1.0;
</pre><h2>Define x<a name="3"></a></h2><p>In this example, we use uniformly spaced x values</p><pre class="codeinput">x = [-5:2:5];
N = length(x);
</pre><h2>Create the training covariance matrix<a name="4"></a></h2><p>Firstly for the training data</p><pre class="codeinput">C = zeros(N);
C = alpha*exp(-gamma*(repmat(x,N,1) - repmat(x',1,N)).^2);
</pre><h2>Generate a true function from the GP prior with zero mean<a name="5"></a></h2><p>Define the GP mean</p><pre class="codeinput">mu = zeros(N,1);
<span class="comment">% Sample a function</span>
true_f = mvnrnd(mu,C);
</pre><h2>Define the test points - again on a uniform grid<a name="6"></a></h2><pre class="codeinput">testx = [-4,-2,0,2,4];
</pre><h2>Plot the training data and show the position of the test points<a name="7"></a></h2><pre class="codeinput">figure();
hold <span class="string">off</span>
plot(x,true_f,<span class="string">'ro'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2)
xlabel(<span class="string">'x'</span>);
ylabel(<span class="string">'f(x)'</span>);
hold <span class="string">on</span>
<span class="comment">% Change the ylimits to make it look nicer</span>
yl = ylim;
yl(1) = 0.9*yl(1);
yl(2) = 1.1*yl(2);
ylim(yl);
xlim([-6,6])
<span class="comment">% Draw dashed lines at the test points</span>
<span class="keyword">for</span> i = testx
    plot([i i],[yl(1) yl(2)],<span class="string">'k--'</span>);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="gppred_01.png" alt=""> <h2>Compute the test covariance<a name="8"></a></h2><p>We need two matrices, <img src="gppred_eq09489741003348511493.png" alt="$\mathbf{C}^*$"> and <img src="gppred_eq08522258881804510052.png" alt="$\mathbf{R}$"> (see page 285)</p><pre class="codeinput">Ntest = length(testx);
<span class="comment">% The train by test matrix</span>
R = alpha*exp(-gamma*(repmat(x',1,Ntest) - repmat(testx,N,1)).^2);
<span class="comment">% The test by test matric</span>
Cstar = alpha*exp(-gamma*(repmat(testx,Ntest,1) - repmat(testx',1,Ntest)).^2);
</pre><h2>Compute the mean and covariance of the predictions<a name="9"></a></h2><p>This uses equation 8.3, p.285</p><pre class="codeinput">figure()
hold <span class="string">off</span>
<span class="comment">% Plot the training data</span>
plot(x,true_f,<span class="string">'ro'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2)
<span class="comment">% Compute the mean and covariance</span>
pred_mu = R'*inv(C)*true_f';
pred_cov = Cstar - R'*inv(C)*R;
<span class="comment">% Extract the standard deviations at the test points (square root of the</span>
<span class="comment">% diagonal elements of the covariance matrix)</span>
pred_sd = sqrt(diag(pred_cov));
hold <span class="string">on</span>
<span class="comment">% Plot the predictions as error bars</span>
errorbar(testx,pred_mu,pred_sd,<span class="string">'ko'</span>,<span class="string">'linewidth'</span>,2,<span class="string">'markersize'</span>,10)
xlim([-6 6]);
xlabel(<span class="string">'x'</span>)
ylabel(<span class="string">'f(x)'</span>)
</pre><img vspace="5" hspace="5" src="gppred_02.png" alt=""> <h2>Plot a smooth predictive function with a one sd window,<a name="10"></a></h2><pre class="codeinput">figure()
hold <span class="string">off</span>
<span class="comment">% plot the data</span>
plot(x,true_f,<span class="string">'ro'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2)
hold <span class="string">on</span>
<span class="comment">% Define a new set of test x values</span>
testx = [-5:0.1:5];
Ntest = length(testx);

<span class="comment">% Compute R and Cstar again for the new test points</span>
R = exp(-gamma*(repmat(x',1,Ntest) - repmat(testx,N,1)).^2);
Cstar = exp(-gamma*(repmat(testx,Ntest,1) - repmat(testx',1,Ntest)).^2);

<span class="comment">% Compute the predictive mean and covariance</span>
pred_mu = R'*inv(C)*true_f';
pred_cov = Cstar - R'*inv(C)*R;

<span class="comment">% Plot the predicted mean and plus and minus one sd</span>
plot(testx,pred_mu,<span class="string">'k'</span>,<span class="string">'linewidth'</span>,2)
plot(testx,pred_mu + sqrt(diag(pred_cov)),<span class="string">'k--'</span>)
plot(testx,pred_mu - sqrt(diag(pred_cov)),<span class="string">'k--'</span>)

xlabel(<span class="string">'x'</span>)
ylabel(<span class="string">'f(x)'</span>)
</pre><pre class="codeoutput">Warning: Imaginary parts of complex X and/or Y arguments ignored 
Warning: Imaginary parts of complex X and/or Y arguments ignored 
</pre><img vspace="5" hspace="5" src="gppred_03.png" alt=""> <h2>Finally, plot some sample functions drawn from the predictive distribution<a name="11"></a></h2><pre class="codeinput">figure(4)
hold <span class="string">off</span>
<span class="comment">% Plot the data</span>
plot(x,true_f,<span class="string">'ro'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2)
hold <span class="string">on</span>
xlim([-6 6])
<span class="comment">% Draw 10 samples from the multivariate gaussian</span>
f_samps = mvnrnd(pred_mu,pred_cov,10);
<span class="comment">% plot them</span>
plot(testx,f_samps,<span class="string">'k'</span>,<span class="string">'color'</span>,[0.6 0.6 0.6])
<span class="comment">% plot the original training data</span>
plot(x,true_f,<span class="string">'ro'</span>,<span class="string">'markersize'</span>,10,<span class="string">'linewidth'</span>,2)

xlabel(<span class="string">'x'</span>)
ylabel(<span class="string">'f(x)'</span>)
</pre><img vspace="5" hspace="5" src="gppred_04.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% gpprior.m
% Performs noise-free GP predictions
%
% From A First Course in Machine Learning
% Simon Rogers, August 2016 [simon.rogers@glasgow.ac.uk]
clear all; close all;

%% Set the kernel parameters
% Try varying these to see the effect
gamma = 0.5;
alpha = 1.0;

%% Define x
% In this example, we use uniformly spaced x values
x = [-5:2:5];
N = length(x);

%% Create the training covariance matrix
% Firstly for the training data
C = zeros(N);
C = alpha*exp(-gamma*(repmat(x,N,1) - repmat(x',1,N)).^2);

%% Generate a true function from the GP prior with zero mean
% Define the GP mean
mu = zeros(N,1);
% Sample a function
true_f = mvnrnd(mu,C);
%% Define the test points - again on a uniform grid
testx = [-4,-2,0,2,4];

%% Plot the training data and show the position of the test points
figure();
hold off
plot(x,true_f,'ro','markersize',10,'linewidth',2)
xlabel('x');
ylabel('f(x)');
hold on
% Change the ylimits to make it look nicer
yl = ylim;
yl(1) = 0.9*yl(1);
yl(2) = 1.1*yl(2);
ylim(yl);
xlim([-6,6])
% Draw dashed lines at the test points 
for i = testx
    plot([i i],[yl(1) yl(2)],'kREPLACE_WITH_DASH_DASH');
end

%% Compute the test covariance
% We need two matrices, $\mathbf{C}^*$ and $\mathbf{R}$ (see page 285)
Ntest = length(testx);
% The train by test matrix
R = alpha*exp(-gamma*(repmat(x',1,Ntest) - repmat(testx,N,1)).^2);
% The test by test matric
Cstar = alpha*exp(-gamma*(repmat(testx,Ntest,1) - repmat(testx',1,Ntest)).^2);

%% Compute the mean and covariance of the predictions
% This uses equation 8.3, p.285
figure()
hold off
% Plot the training data
plot(x,true_f,'ro','markersize',10,'linewidth',2)
% Compute the mean and covariance
pred_mu = R'*inv(C)*true_f';
pred_cov = Cstar - R'*inv(C)*R;
% Extract the standard deviations at the test points (square root of the 
% diagonal elements of the covariance matrix)
pred_sd = sqrt(diag(pred_cov));
hold on
% Plot the predictions as error bars
errorbar(testx,pred_mu,pred_sd,'ko','linewidth',2,'markersize',10)
xlim([-6 6]);
xlabel('x')
ylabel('f(x)')
%% Plot a smooth predictive function with a one sd window,
figure()
hold off
% plot the data
plot(x,true_f,'ro','markersize',10,'linewidth',2)
hold on
% Define a new set of test x values
testx = [-5:0.1:5];
Ntest = length(testx);

% Compute R and Cstar again for the new test points 
R = exp(-gamma*(repmat(x',1,Ntest) - repmat(testx,N,1)).^2);
Cstar = exp(-gamma*(repmat(testx,Ntest,1) - repmat(testx',1,Ntest)).^2);

% Compute the predictive mean and covariance
pred_mu = R'*inv(C)*true_f';
pred_cov = Cstar - R'*inv(C)*R;

% Plot the predicted mean and plus and minus one sd
plot(testx,pred_mu,'k','linewidth',2)
plot(testx,pred_mu + sqrt(diag(pred_cov)),'kREPLACE_WITH_DASH_DASH')
plot(testx,pred_mu - sqrt(diag(pred_cov)),'kREPLACE_WITH_DASH_DASH')

xlabel('x')
ylabel('f(x)')

%% Finally, plot some sample functions drawn from the predictive distribution
figure(4)
hold off
% Plot the data
plot(x,true_f,'ro','markersize',10,'linewidth',2)
hold on
xlim([-6 6])
% Draw 10 samples from the multivariate gaussian
f_samps = mvnrnd(pred_mu,pred_cov,10);
% plot them
plot(testx,f_samps,'k','color',[0.6 0.6 0.6])
% plot the original training data
plot(x,true_f,'ro','markersize',10,'linewidth',2)

xlabel('x')
ylabel('f(x)')

##### SOURCE END #####
--></body></html>