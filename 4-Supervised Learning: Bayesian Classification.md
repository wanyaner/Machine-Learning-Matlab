# 4-Supervised Learning: Bayesian Classification

## Classification
* Generative (e.g. Bayesian)
* Instance-based (e.g. k-NN)
* Discriminative (e.g. SVM)

A set of N objects with attributes Xn;  
Each object has an associated target label tn.  


## Probabilistic vs non-Probabilistic classification

probabilistic classifiers: P(tnew = k | xnew, X, t)

<br>


## Generative vs discriminative classification

<br>
Generative classifiers generate a model for each class, based on training samples available.  

* Data in each class can be seen as generated by some model.
* For new test samples, they assign these samples to the class that suits best (e.g. by probability measure)

Discriminative classifiers attempt to explicitly define the decision boundary that separates the classes.

* Intuitively, these methods are for binary class problems but can be extended to multi-class problems.


## Bayesian Classifier
<br>
A classifier built on Bayes rule.

* Builds a probabilistic model of the data, embeddingprior knowledge
* Allows us to extract prior knowledge from observed data
Generative approach
* Builds a model from training objects
* Any new objects can be classified based on the probabilistic model specification
## Probability
**conditional probability**
* P(Y=y| X=x):probability of y, given that ... x
**Joint probability**
* P(Y=y, X=x) = P(Y = y)P(X = x)
* P(Y=y, X=x) = P(Y=y| X=x)P(X=x), or  
  P(Y=y, X=x) = P(X=x| Y=y)P(Y=y)
**Marginal probability**
* ğ‘ƒ(ğ‘Œ=ğ‘¦) = ğ‘ƒ(ğ‘Œ=ğ‘¦,ğ‘‹=0) + ğ‘ƒ(ğ‘Œ=ğ‘¦,ğ‘‹=1)    1 = P(Y=0) + P(Y=1)  
  ğ‘‹ has been marginalized from the joint distribution
**Bayes rule**
* P(X=x| Y=y) = P(Y=y| X=x)P(X=x)/P(Y=y)  
![image](https://github.com/wanyaner/MarkdownPic/blob/master/Bayes-rule.png?raw=true)
---## Bayesian Classification

![image](https://github.com/wanyaner/MarkdownPic/blob/master/bayessian-classifier-explaination.png?raw=true)
  
* **Maximum a posterior(MAP)** hypothesis
  arg max P(Xnew|c)P(c) c âˆˆ H  
  Typically, MAP estimate is used for Bayesian classification since it is flexible regarding prior use
  
* **masimum likelihood(ML)** hypothesis
  arg max P(Xnew|c) c âˆˆ H  
![image](https://github.com/wanyaner/MarkdownPic/blob/master/bayessian-classifier.png?raw=true)  

## Naive Bayes Assumption
NaiÌˆve Bayes assumption: attributes that describe data are conditionally independent given a hypothesis:  
![image](https://github.com/wanyaner/MarkdownPic/blob/master/naive-bayes-classifier.png?raw=true)  

* note that in MAP P(Xnew|c)denotes probability for the I attribute value;
* each attribute has discrete values(in discussion so far), very simple but practical classification algorithm
* successful applications: Medical diagnosis, Tect classification.

## Gaussian classification
* For **discrete-valued attributes**, we used probability distributions to estimate attribute value relation with class label
* For **real-valued attributes**, we need to use **probability density function (pdf)** to estimate attribute value relation with class label
* Gaussian classifier: Bayes or NaiÌˆve Bayes classifier that utilizes Gaussian pdf  

![image](https://github.com/wanyaner/MarkdownPic/blob/master/Gaussian-classification.png?raw=true)  


**Making predictions:**

* By evaluating the Gaussian classifier on a grid of many ğ’™ğ‘›ğ‘’ğ‘¤ values, we can estimate and draw the **classification probability contours**

**Automatic tet classification**

* attributes:Bag-of-words, Word-frequency

